{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import load_conversation_template\n",
    "template_name = 'llama-2'\n",
    "conv_template = load_conversation_template(template_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.modelUtils import *\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from casper import nethook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name =\"gpt2\"  # or \"Llama2-7B\" or \"EleutherAI/gpt-neox-20b\"\n",
    "mt = ModelAndTokenizer(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=False,\n",
    "    torch_dtype=(torch.float16 if \"20b\" in model_name else None),\n",
    "    device = 'cuda:0'\n",
    ")\n",
    "mt.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"What is your name?\"\n",
    "predict_token(\n",
    "    mt,\n",
    "    [test_prompt],\n",
    "    return_p=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_outputs(test_prompt,mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_with_patch_layer(\n",
    "    model,  # The model\n",
    "    inp,  # A set of inputs\n",
    "    states_to_patch,  # A list of (token index, layername) triples to restore\n",
    "    answers_t,  # Answer probabilities to collect  \n",
    "):\n",
    "    prng = np.random.RandomState(1)  # For reproducibility, use pseudorandom noise\n",
    "    layers = [states_to_patch[0], states_to_patch[1]]\n",
    "\n",
    "    # Create dictionary to store intermediate results\n",
    "    inter_results = {}\n",
    "\n",
    "    def untuple(x):\n",
    "        return x[0] if isinstance(x, tuple) else x\n",
    "\n",
    "    # Define the model-patching rule.\n",
    "    def patch_rep(x, layer):\n",
    "        if layer not in layers:\n",
    "            return x\n",
    "\n",
    "        if layer == layers[0]:\n",
    "            inter_results[\"hidden_states\"] = x[0].cpu()\n",
    "            inter_results[\"attention_mask\"] = x[1][0].cpu()\n",
    "            inter_results[\"position_ids\"] = x[1][1].cpu()\n",
    "            return x\n",
    "        elif layer == layers[1]:\n",
    "            short_cut_1 = inter_results[\"hidden_states\"].cuda()\n",
    "            short_cut_2_1 = inter_results[\"attention_mask\"].cuda()\n",
    "            short_cut_2_2 = inter_results[\"position_ids\"].cuda()\n",
    "            short_cut_2 = (short_cut_2_1, short_cut_2_2)\n",
    "            short_cut = (short_cut_1, short_cut_2)\n",
    "            return short_cut\n",
    "            \n",
    "    with torch.no_grad(), nethook.TraceDict(\n",
    "        model,\n",
    "        layers,\n",
    "        edit_output=patch_rep,\n",
    "    ) as td:\n",
    "        outputs_exp = model(**inp)\n",
    "\n",
    "    probs = torch.softmax(outputs_exp.logits[1:, -1, :], dim=1).mean(dim=0)[answers_t]\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def analyse_based_on_layer(prompt,):\n",
    "    inp = make_inputs(mt.tokenizer,[prompt]*2)\n",
    "    with torch.no_grad():\n",
    "        answer_t, logits = [d[0] for d in predict_from_input(mt.model, inp)]\n",
    "    [answer] = decode_tokens(mt.tokenizer, [answer_t])\n",
    "    print(answer)\n",
    "    model = mt.model\n",
    "    result_prob = []\n",
    "    for layer in range(mt.num_layers-1):\n",
    "        layers = [layername(model, layer),layername(model, layer + 1)]\n",
    "        print(layers)\n",
    "        prob =  trace_with_patch_layer(model, inp, layers,answer_t)\n",
    "        result_prob.append(prob)\n",
    "     # Convert tensors to a list of numbers\n",
    "    data_on_cpu = [abs(x.item() - logits.item()) for x in result_prob]\n",
    "    # Create a list of indices for x-axis\n",
    "        \n",
    "    return logits.item() ,data_on_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, layerAIE = analyse_based_on_layer(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "seq = layerAIE\n",
    "logits = logits\n",
    "kurt = kurtosis(seq, fisher=False)\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "\n",
    "sns.scatterplot(x=range(1, len(seq)+1), y=seq, color='b')\n",
    "\n",
    "plt.title('Prompt: ' + test_prompt)\n",
    "\n",
    "\n",
    "plt.figtext(0.3, 0.03, f'Logits: {logits:.4f}', ha='center', va='center')\n",
    "plt.figtext(0.7, 0.03, f'Kurtosis: {kurt:.4f}', ha='center', va='center')\n",
    "# plt.savefig(\"M:\\Causallm-attack\\paper\\\\figure\\pdf\\\\adver_13b.pdf\",bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_with_patch_neuron(\n",
    "    model,  # The model\n",
    "    inp,  # A set of inputs\n",
    "    layers,  # what layer to perform causlity analysis\n",
    "    neuron_zone, # zone of neurons\n",
    "    answers_t,  # Answer probabilities to collect  \n",
    "):\n",
    "\n",
    "    prng = np.random.RandomState(1)  # For reproducibility, use pseudorandom noise\n",
    "    layer = layers[0]\n",
    "    start_neuron =  neuron_zone[0]\n",
    "    end_neuron = neuron_zone[1]\n",
    "    def untuple(x):\n",
    "        return x[0] if isinstance(x, tuple) else x\n",
    "\n",
    "    # Define the model-patching rule.\n",
    "    def patch_rep(x, layer):\n",
    "        if layer != layer:\n",
    "            return x\n",
    "\n",
    "        if layer == layer:\n",
    "            h = untuple(x)\n",
    "            zeros = torch.zeros_like(h)\n",
    "            h[:, :, start_neuron:end_neuron] =  zeros[:, :, start_neuron:end_neuron] \n",
    "            x_2_1 = x[1][0]\n",
    "            x_2_2 = x[1][1]\n",
    "            result = (h,(x_2_1,x_2_2))\n",
    "           \n",
    "            return result\n",
    "\n",
    "    with torch.no_grad(), nethook.TraceDict(\n",
    "        model,\n",
    "        layers,\n",
    "        edit_output=patch_rep,\n",
    "    ) as td:\n",
    "        outputs_exp = model(**inp)\n",
    "    \n",
    "    probs = torch.softmax(outputs_exp.logits[1:, -1, :], dim=1).mean(dim=0)[answers_t]\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def analysed_based_on_neuron(prompt, mt, analyse_layer,analysed_neurons, save_numpy = None, ):\n",
    "    inp = make_inputs(mt.tokenizer,[prompt]*2,)\n",
    "    with torch.no_grad():\n",
    "        answer_t, logits = [d[0] for d in predict_from_input(mt.model, inp)]\n",
    "    [answer] = decode_tokens(mt.tokenizer, [answer_t])\n",
    "    \n",
    "    result_prob = []\n",
    "    for zone_index in tqdm(analysed_neurons):\n",
    "        layers = [layername(mt.model, analyse_layer)]\n",
    "                # print(layers)\n",
    "        neuron_zone = [zone_index ,(zone_index+1)]\n",
    "        prob = trace_with_patch_neuron(mt.model, inp, layers,neuron_zone,answer_t)\n",
    "        result_prob.append(prob)\n",
    "\n",
    "    data_on_cpu = [abs(logits.item() - x.item()) for x in result_prob]\n",
    "        # print(data_on_cpu)\n",
    "    if save_numpy is not None:\n",
    "        np.save(save_numpy,data_on_cpu)\n",
    "        \n",
    "        \n",
    "    return logits.item() , data_on_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, neuronAIE = analysed_based_on_neuron(test_prompt,mt, 0,range(768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = neuronAIE\n",
    "aieRange = np.max(seq) - np.min(seq)\n",
    "\n",
    "# 设定 seaborn 的样式\n",
    "sns.set_theme()\n",
    "plt.figure(dpi=1000)\n",
    "plt.figure(figsize=(9,6))\n",
    "\n",
    "\n",
    "sns.scatterplot(x=range(0, len(seq)), y=seq, color='b' ,s =20)\n",
    "\n",
    "plt.title('Layer')\n",
    "\n",
    "\n",
    "plt.figtext(0.3, 0, f'Logits: {logits:.4f}', ha='center', va='center')\n",
    "plt.figtext(0.7, 0, f'Range: {kurt:.4f}', ha='center', va='center')\n",
    "plt.annotate(f'({np.argmax(seq)},{str(seq[np.argmax(seq)])[:5]})', (np.argmax(seq), seq[np.argmax(seq)]), textcoords=\"offset points\", xytext=(-2,-15), ha='center')\n",
    "plt.annotate(f'({2100},{str(seq[2100])[:5]})', (2100, seq[2100]), textcoords=\"offset points\", xytext=(-2,-15), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"M:\\Causallm-attack\\paper\\\\figure\\pdf\\\\13b_neuron_2_adv.pdf\",bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
